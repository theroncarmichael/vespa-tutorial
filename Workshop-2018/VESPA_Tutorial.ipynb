{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of Exoplanet Signals using a Probabilistic Algorithm (VEPSA) Tutorial\n",
    "*Tutorial created by Tim Morton (VESPA code author), Juliette Becker, Andrew Vanderburg*\n",
    "\n",
    "----\n",
    "\n",
    "This tutorial cracks open the black box of [vespa](vespa.rtfd.org), which is an implementation of the procedure described in [Morton (2012)](https://arxiv.org/abs/1206.1568) to compute the false positive probability of a transiting planet candidate.  \n",
    "\n",
    "If a directory is [completely prepared](#Preparing-a-vespa-directory), running `vespa` consists of executing the following commands from a terminal:\n",
    "\n",
    "    starfit --all <directory name>\n",
    "    calcfpp <directory name>\n",
    "\n",
    "The `starfit --all` command [estimates the stellar properties of the host star](#Fitting-stellar-models), and `calcfpp` [computes the false positive probability](#Calculating-FPP).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a vespa directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a `vespa` calculation requires creating `star.ini` and `fpp.ini` config files, as described below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host star\n",
    "\n",
    "Create a `star.ini` file containing the RA/Dec coordinates of the transit candidate host star, and any available observed properties, such as broadband photometric magnitudes, spectroscopic properties, or parallax.\n",
    "All quantities are listed as `value, uncertainty`.\n",
    "One of the magnitudes provided must be the band in which the transit was observed, and need not have an uncertainty (values without uncertainties will not be used in the model fitting).\n",
    "Here is an example `star.ini` file:\n",
    "\n",
    "    ra = 289.217499\n",
    "    dec = 47.88446\n",
    "    J = 10.523, 0.02\n",
    "    H = 10.211, 0.02\n",
    "    K = 10.152, 0.02\n",
    "    g = 12.0428791, 0.05\n",
    "    r = 11.5968652, 0.05\n",
    "    i = 11.4300704, 0.05\n",
    "    z = 11.393061, 0.05\n",
    "    Kepler = 11.664\n",
    "    Teff = 5642, 50.0\n",
    "    feh = -0.27, 0.08\n",
    "    logg = 4.443, 0.028"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planet candidate\n",
    "\n",
    "Create a text file containing the photometry of the detected candidate, detrended and phase-folded.\n",
    "This file should have the following three columns in order:\n",
    "\n",
    "  * Time from mid-transit, in units of days\n",
    "  * Relative flux, normalized to unity; e.g., `flux / median(flux)`.\n",
    "  * Relative flux uncertainty (also normalized)\n",
    "\n",
    "The photometry should be limited to only those points within just a few transit durations of the transit, not the entire orbital phase.\n",
    "This file may have any name, but for current purposes, let's call it `transit.txt`.\n",
    "\n",
    "For a quick description of how to make a file like this for a new candidate, see [Appendix 1](#Appendix-1) of this notebook. \n",
    "\n",
    "You can check to make sure that this is put together correctly by using the utilities provided in and used by `vespa` to load and visualize the candidate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from vespa import TransitSignal\n",
    "trsig = TransitSignal.from_ascii('TestCase1/transit.txt')\n",
    "trsig.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of three other analyses (calculations that `vespa` does not do) must also be provided:\n",
    "\n",
    "  - A best-fit estimate of the planet/star radius ratio.\n",
    "\n",
    "  - An observational upper limit on the depth of a potential secondary eclipse in the light curve.\n",
    "    This may be calculated by, e.g., running a transit search in the light curve at other phases but keeping the period fixed.\n",
    "\n",
    "  - A limit on the furthest angular separation from the target star that a potential blending star might reside.\n",
    "    This limit should come from pixel-level analysis of the target star photometry, establishing that the signal does not originate from a different star.\n",
    "    While the tightest constraint will come from some kind of centroid or pixel-modeling effort (e.g. [Bryson et al, 2013](https://arxiv.org/pdf/1303.0052.pdf)), it should also be sufficient to test the depth of the signal as a function of aperture size, to see whether the measured depth is aperture-dependent (that is, if the signal is caused by a small amount of flux from a bright eclipsing binary many pixels away from the target, then the signal will be deeper with larger apertures.)\n",
    "    A good example of what can happen if this analysis is not done carefully is with EPIC210400868 from [Cabrera et al., 2017](https://arxiv.org/pdf/1707.08007.pdf).\n",
    "\n",
    "All of this information gets summarized in another config file: `fpp.ini`, as follows:\n",
    "\n",
    "    name = K00087.01\n",
    "    ra = 289.217499\n",
    "    dec = 47.88446\n",
    "    period = 289.864456     # Orbital period of candidate\n",
    "    rprs = 0.021777742485   # Planet/star radius ratio\n",
    "    photfile = transit.txt  # File containing transit photometry\n",
    "\n",
    "    [constraints]\n",
    "    maxrad = 1.05           # Maximum potential blending radius, in arcsec\n",
    "    secthresh = 9.593e-05   # Maximum allowed secondary eclipse depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting stellar models\n",
    "\n",
    "The first step of a `vespa` calculation is to fit the stellar parameters to the observed properties of the star.\n",
    "Before this step, the directory should look like this:\n",
    "\n",
    "    $ ls TestCase1\n",
    "    fpp.ini\n",
    "    star.ini\n",
    "    transit.txt\n",
    "\n",
    "\n",
    "Fitting the stellar properties consists of running the `starfit` script, which is part of the `isochrones` package:\n",
    "\n",
    "Run in a bash terminal:\n",
    "\n",
    "    starfit --all TestCase1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script performs three different fits: single-, binary- and triple-star models.\n",
    "It should take approximately 25 minutes to run: about 3, 7, and 15 minutes for the single, binary, and triple models, respectively.\n",
    "After the script finishes, your directory should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls TestCase1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mist_starmodel_*.h5` files contain the samples from the posterior probability distribution of the model parameters, as well as samples of derived parameters.\n",
    "You can load the stellar model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isochrones import StarModel    \n",
    "mod_single = StarModel.load_hdf('TestCase1/mist_starmodel_single.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the object used to fit the stellar model.  The parameters it fits for are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_single.param_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary star model fits for two stars, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_binary = StarModel.load_hdf('TestCase1/mist_starmodel_binary.h5')\n",
    "mod_binary.param_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can investigate the posterior samples of the model parameters, as well as many derived parameters, via the `.samples` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_single.samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `*.png` files created by `starfit` in the directory contain diagnostic plots.\n",
    "There are two kinds of \"corner\" plots that show the joint distributions of various parameters: `*_physical.png` and `*_observed.png`.\n",
    "The \"physical\" plots show the distribution of the physical parameters of the star(s) resulting from the model fits: mass, radius, age, [Fe/H], distance, and extinction.  (Radius is the only of these that is a derived parameter, rather than a directly fitted parameter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"TestCase1/mist_corner_binary_physical.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the posterior distribution of secondary star mass (`mass_0_1`), and system distance (`distance_0`) is bimodal.  Think about this bimodality.  Can you explain why it is there?\n",
    "\n",
    "Now, look at `mist_corner_triple_observed.png`.  Do you see a similar feature? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"observed\" plots show the distribution of the derived parameters of the model that correspond to the quantities used to constrain the models; in this case, seven photometric bands and three spectroscopic parameters.\n",
    "These figures also show the provided constraint values (blue lines), which can be indicative of a poor stellar model fit if they do not lie comfortably within the distribution of the modeled parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"TestCase1/mist_corner_binary_observed.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating FPP\n",
    "\n",
    "With the stellar model fits complete, you can now calculate the false positive probability by executing the following in a terminal:\n",
    "\n",
    "    $ calcfpp TestCase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to do a quicker test run, you can run `calcfpp -n 1000` (for example), to make smaller populations (the default `n` is 20000, which takes about 10 minutes).\n",
    "\n",
    "[Morton (2012)](http://adsabs.harvard.edu/cgi-bin/nph-data_query?bibcode=2012ApJ...761....6M&link_type=PREPRINT&db_key=AST) describes the procedure that `vespa` uses to calculate the false positive probability (FPP) of a planet candidate.  In short, the calculation is as follows:\n",
    "\n",
    "$$  {\\rm FPP} = 1 - P_{\\rm pl}, $$\n",
    "\n",
    "where\n",
    "\n",
    "$$  P_{\\rm pl} = \\frac{\\mathcal L_{\\rm pl} \\pi_{\\rm pl}}\n",
    "                                 {\\mathcal L_{\\rm pl} \\pi_{\\rm pl} +\n",
    "            \\mathcal L_{\\rm FP} \\pi_{\\rm FP}}. $$\n",
    "            \n",
    "The $\\mathcal L_i$ here represent the \"model likelihood\"\n",
    "factors and the $\\pi_i$ represent the \"model priors,\" with the\n",
    "${\\rm FP}$ subscript representing the sum of $\\mathcal L_i \\pi_i$ for each of the false positive scenarios.  A brief description of how the likelihoods and priors for each of the models are calculated can be found in the [vespa online documentation](http://vespa.readthedocs.io/en/latest/fpp.html).\n",
    "\n",
    "If you follow along with the output of `calcfpp`, you will notice it first fits the trapezoid model to the observed transit signal.\n",
    "It then proceeds to generate populations for lots of different models, and subsequently to fit a trapezoid model to each instance.\n",
    "By default, `calcfpp` will use the following models:\n",
    "\n",
    "  * BEB (background(/foreground) eclipsing binary---physically unassociated with target star)\n",
    "  * HEB (hierarchcial eclipsing binary)\n",
    "  * EB (eclipsing binary---the target star is an EB, no additional blending)\n",
    "  * Pl (planet: the true transiting planet model)\n",
    "\n",
    "There are also `_Px2` versions of the EB models, in which the false positive scenario has a period exactly twice the candidate's period, which could happen if the primary and secondary EB eclipse depths are very similar.\n",
    "\n",
    "After running `calcfpp`, you now have the following files in your directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls TestCase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see `*.h5` and `*.png` files have been created.\n",
    "\n",
    "One interesting file is the `starfield.h5` file, which contains the TRILEGAL simulation of the background population of stars, used in the BEB model population.\n",
    "The purpose of this file is to simulate the stellar photometry of the field.\n",
    "Let's take a look at its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "starfield = pd.read_hdf(\"TestCase1/starfield.h5\",'df')\n",
    "\n",
    "# let's look at the columns of this simulation:\n",
    "starfield.columns  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the quantities simulated in the field. We can also plot the HR diagram of all the objects in the field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(starfield['logTe'], starfield[u'logL'],marker='.',\n",
    "     c = starfield[u'Kepler_mag'],s=1)\n",
    "plt.gca().invert_xaxis()\n",
    "plt.colorbar(label=\"Kepler Magnitude\")\n",
    "plt.xlabel(\"Log($T_{eff}$ / K)\", fontsize=20)\n",
    "plt.ylabel(\"Log(L / L$_{sun}$)\", fontsize=20)\n",
    "\n",
    "# So, what we see here is the HR diagram for the simulated star field. \n",
    "\n",
    "# more details on how simulation is run:\n",
    "#  http://stev.oapd.inaf.it/~webmaster/trilegal_1.6/help.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`popset.h5` contains the simulated populations, and can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa import PopulationSet\n",
    "popset = PopulationSet.load_hdf('TestCase1/popset.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual populations can be accessed from this object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebs = popset['eb']\n",
    "bebs = popset['beb']\n",
    "hebs = popset['heb']\n",
    "pls = popset['pl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these population objects has a `.stars` attribute that contains all of the data for all the simulated instances of that model.  Investigate this `.stars` dataframe a bit.  For two different populations, make scatter plots of different columns to see if you can see interesting distributions.  You may wish to pay special attention to the `depth`, `duration`, and `slope` columns, which are the parameters of the trapezoid shape model. The different distributions of these parameters for the different populations is what allows us to distinguish between planet and false positive models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, `*.png` files are diagnostic figures.  `FPPsummary.png` displays the summary of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"TestCase1/FPPsummary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The others are informative visualizations of the various models, showing the distribution of simulated trapezoidal model parameters compared to the trapezoidal fit to the true transit candidate signal; for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('TestCase1/eb.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('TestCase1/pl.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also directly load the `FPPCalculation` object from this directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa import FPPCalculation\n",
    "fpp = FPPCalculation.load('TestCase1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should be able to quickly get the false positive probability result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.FPP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation is quick this time because the populations are already generated, and the likelihood computations have been cached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 1\n",
    "### Preparing .ini files for your own system(s)\n",
    "As noted above, VESPA requires a detrended, flatened, phase-folded lightcurve to work. [As described here](https://github.com/barentsen/kepler-athenaeum-tutorial/blob/master/how-to-find-a-planet-tutorial.ipynb), the process of finding a planet requires several steps before you get to the point where you have a phase-folded, flattened light curve (like the kind you need for VESPA). Let's try the steps [from this tutorial](https://github.com/barentsen/kepler-athenaeum-tutorial/blob/master/how-to-find-a-planet-tutorial.ipynb) and flatten a light curve.\n",
    "\n",
    "First, let's take a sample light curve from the K2 mission. Let's try the one at [this URL](https://www.cfa.harvard.edu/~avanderb/k2c14/ep248463350.html), EPIC 248463350. You can download it from the website, or you can use the file `Data/raw248463350.txt` which is available on your machine.\n",
    "\n",
    "The ExoFop page for this source is [at this link](https://exofop.ipac.caltech.edu/k2/edit_target.php?id=248463350). \n",
    "You will need some of the stellar parameters and imaging observations to make the `star.ini` file. \n",
    "\n",
    "Save the files you will need on this target (or a different one, if you repeat the process) to your Data directory, and let's plot it and see what it looks like:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the transit.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "epic_target = pd.read_csv(\"Data/raw248463350.txt\", skiprows=1,names=['time','flux', 'error'])\n",
    "plt.errorbar(epic_target['time'], epic_target['flux'],epic_target['error'])\n",
    "plt.ylabel(\"Flux\", fontsize=20)\n",
    "plt.xlabel(\"Time\", fontsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This light curve has long term trends (what could those be due to?). For finding planets, they are not very helpful. So, we will use a low pass filter to remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "trend = scipy.signal.savgol_filter(epic_target['flux'], 101, polyorder=3) \n",
    "epic_target['corr_flux'] = (epic_target['flux'] / trend)\n",
    "\n",
    "plt.plot(epic_target['time'], epic_target['corr_flux'], '.')\n",
    "plt.ylabel(\"Flux (flattened)\", fontsize=18)\n",
    "plt.xlabel(\"Time\", fontsize=18)\n",
    "plt.ylim(0.9978, 1.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that this low pass filter is not perfect (the bumps near each transit arise because we did not remove near transit points, and without removing those points we can't use this curve for real science, only for a quick look at what's going on in the light curve), but it has turned the messy light curve into a flattened curve with a few events that look like transits. At this point, we need to figure out the periods of the planets, so we know which periods to fold over to generate the `transit.ini` files for VESPA. For details on how to do this, you can go to the EXOFAST tutorial at this workshop (or read about it online [here](https://exoplanetarchive.ipac.caltech.edu/docs/exofast/exofast_recipe_2.html)). Let's assume that you asked your collaborator (who attended the EXOFAST workshop) to do the fit for you, and they sent back the following parameters:\n",
    "\n",
    "| Quantity | Planet 1 | Planet 2 |\n",
    "|------|------|------|\n",
    "|   Period (days)  | 6.393653 | 18.788228|\n",
    "|   Time of center transit (days)  | 2457941.008880 | 2457930.470510 |\n",
    "|   R_p/R_star (fractional)  | 0.036485216 | 0.017301727 |\n",
    "|Transit Duration (days)| 0.14959668| 0.18613987|\n",
    "\n",
    "\n",
    "Using those parameters, you now know that there are two planets in the system. Your collaborator also sent back a flattened light cruve with different columns corrsponding to different planets (with events where multiple transits occur simultaneously excluded), which we will use going forward in place of the imperfect low pass filter from before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values your collaborator sent:\n",
    "planet_no = [2,1]\n",
    "periods = [6.393653, 18.788228] #days\n",
    "tcent = [2457941.008880,2457930.470510] # days\n",
    "rp_rs = [0.017301727,0.036485216] # fractional\n",
    "durations = [0.14959668, 0.18613987] #days\n",
    "\n",
    "# Your collaborator from the EXOFAST workshop also sent you this version of the lightcurve, where they\n",
    "# removed the transit events that occur at the same time (last two columns) for use with VESPA.\n",
    "headings = ['Time', 'Relative Flux', 'Flattened Relative Flux', 'Flattened Relative Flux with planet 2 removed',\n",
    "           'Flattened Relative Flux with planet 1 removed']\n",
    "epic_target_fitted = pd.read_csv(\"Data/processed248463350.csv\", delimiter=\",\", skiprows=1,names = headings,engine='python',index_col = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try plotting the modded curves for planet(s) in the system. We will take a look at one planet (the first one, with the 6 day period) to start with, and you can try the other on your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epic_target_fitted['Time'].values % periods[0], epic_target_fitted['Flattened Relative Flux with planet 1 removed'].values, '.')\n",
    "plt.ylabel(\"Flux (flattened)\", fontsize=18)\n",
    "plt.xlabel(\"Time\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good, because we now have a folded, flattened light curve! However, take a look back at the transit curve in the example in this notebook. We don't really need all that flat continuum, so let's trim the transit around its center. Make sure to leave continuum equal to about the duration of the transit event on each side of the transit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: if you look in the raw fit file, you'll notice the times given are BJD - 2454833. \n",
    "phased_time = (epic_target_fitted['Time'].values  - tcent[0] + 2454833  +periods[0] / 2 ) % periods[0] - periods[0] / 2\n",
    "epic_target_fitted['Phased Time'] = phased_time\n",
    "idx = (epic_target_fitted['Phased Time'] > -0.5) & (epic_target_fitted['Phased Time'] < 0.5)\n",
    "plt.plot(epic_target_fitted.loc[idx]['Phased Time'], epic_target_fitted.loc[idx]['Flattened Relative Flux with planet 1 removed'].values, '.')\n",
    "plt.ylabel(\"Flux (flattened)\", fontsize=18)\n",
    "plt.xlabel(\"Time\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to write this curve to `transit.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame({\"1\":epic_target_fitted.loc[idx]['Phased Time'], \n",
    "                       \"2\":epic_target_fitted.loc[idx]['Flattened Relative Flux with planet 1 removed'].values},\n",
    "                      )\n",
    "new_df.to_csv(\"Example/transit.txt\", index = False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the fpp.ini file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, the `fpp.ini` files looked something like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "less TestCase1/fpp.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to make a similar file, which includes all known constraints on this system, for EPIC 248463350. You can find a lot of the information that you need at the ExoFop page for this source [here](https://exofop.ipac.caltech.edu/k2/edit_target.php?id=248463350). \n",
    "\n",
    "Try creating this file, including the information from ExoFop and the values your collaborator sent you. \n",
    "\n",
    "The first set of parameters (`name`, `ra`, `dec`) you can get from ExoFop. \n",
    "\n",
    "`Rprs` (the radius of the planet over the radius of the star) and the `period` should be derived from the fit to the lightcurve. \n",
    "\n",
    "`Maxrad` is the aperture radius in arcsec. To find this value, we first need to know how many pixels are in the photometric aperture, which requires knowing what the aperture looks like. Go to [this page](https://www.cfa.harvard.edu/~avanderb/k2c14/ep248463350.html) and scroll to the bottom, and look at the pixel file (which is also reproduced in the next cell). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('Data/pixelfile.png')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the pixels in the aperture.  Then, since the Kepler platescale is 3.98 arcseconds per pixel, convert to arcseconds and solve for the radius of the aperture. Once that radius is found, add the Kepler PSF (6 arcseconds) to be safe, and you have the value for `maxrad` that should be added to `fpp.ini`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = 21\n",
    "arsecsq = pixels * 3.98**2.0\n",
    "radius = np.sqrt(arsecsq / np.pi)\n",
    "effective_radius = radius + 6\n",
    "print(\"MAXRAD:\", effective_radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**\n",
    "\n",
    "It is very important to understand that the `vespa` calculation assumes that there are no other known stars other than the target star within this aperture.  If there *are* other known stars and you ignore this fact, then you run the risk of underestimating the FPP of the transit candidate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Next* - `secthresh` is the maximum allowed depth of potential secondary eclipse. This should be computed from the lightcurve. Following the method in [Morton et al. (2016)](https://arxiv.org/pdf/1605.02825.pdf), this can be derived by searching the phased-folded light curve for the deepest signal at any other phase other than that of the primary transit. See also Section 3.2.2 of [Rowe et al. (2015)](http://iopscience.iop.org/article/10.1088/0067-0049/217/1/16/pdf). \n",
    "\n",
    "In this tutorial, we implement a simplified version of this method by taking the light curve, phase folding it with our fitted orbital period, and checking the depth of the expected bottom of transit near the expected center of secondary eclipse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the phased light curve again: \n",
    "plt.plot(epic_target_fitted['Phased Time'], epic_target_fitted['Flattened Relative Flux with planet 1 removed'].values, '.')\n",
    "plt.ylabel(\"Flux (flattened)\", fontsize=18)\n",
    "plt.xlabel(\"Time (phased, days)\", fontsize=18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, look at the event depth at locations near where you expect the secondary transit event to be, keeping all other light curve parameters the same. In the plot below, we highlight the entire region where a secondary transit would occur, if it were visible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: if you look in the raw fit file, you'll notice the times given are BJD - 2454833. \n",
    "offset_phased_time = (epic_target_fitted['Time'].values  - tcent[0] + 2454833 ) % periods[0] - periods[0] / 2\n",
    "epic_target_fitted['Offset Time'] = offset_phased_time\n",
    "idx = (epic_target_fitted['Offset Time'] > -0.5*durations[0]) & (epic_target_fitted['Offset Time'] < 0.5*durations[0])\n",
    "plt.plot(epic_target_fitted.loc[idx]['Offset Time'], epic_target_fitted.loc[idx]['Flattened Relative Flux with planet 1 removed'].values,'.', color='k',  label=\"Where Secondary should be\")\n",
    "plt.plot(epic_target_fitted['Offset Time'], epic_target_fitted['Flattened Relative Flux with planet 1 removed'].values, '.', color='k',alpha=0.1, label=\"all data\")\n",
    "plt.ylabel(\"Flux (flattened)\", fontsize=18)\n",
    "plt.xlabel(\"Time (phased, days)\", fontsize=18)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the plot above highlights the entire duration. What we really want to check is the average flux during the true eclipse event, excluding ingress and egress. So, splitting up the lightcurve near the location of the secondary event and taking the mean of only the 'flat' region of the eclipse should yield the limit on how deep the secondary eclipse could be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ineg = (epic_target_fitted['Offset Time'] > -0.5*durations[0]) & (epic_target_fitted['Offset Time'] < 0.5*durations[0])\n",
    "idx_event = (epic_target_fitted['Offset Time'] > -0.25*durations[0]) & (epic_target_fitted['Offset Time'] < 0.25*durations[0])\n",
    "plt.plot(epic_target_fitted.loc[idx_ineg]['Offset Time'], epic_target_fitted.loc[idx_ineg]['Flattened Relative Flux with planet 1 removed'].values,'.', color='r', alpha=0.5, label=\"(Expected) ingress/egress, start of event\")\n",
    "plt.plot(epic_target_fitted.loc[idx_event]['Offset Time'], epic_target_fitted.loc[idx_event]['Flattened Relative Flux with planet 1 removed'].values,'.', color='DarkBlue',  alpha=0.5, label=\"(Expected) transit\")\n",
    "t = epic_target_fitted.loc[idx_event]['Offset Time']\n",
    "mean_flux = mean(epic_target_fitted.loc[idx_event]['Flattened Relative Flux with planet 1 removed'].values)\n",
    "plt.plot(t,len(t) * [mean_flux], label=\"Mean flux level in event\", color='DarkBlue')\n",
    "plt.ylabel(\"Flux (flattened)\", fontsize=18)\n",
    "plt.xlabel(\"Time (T$_{sec, expected}$ - t, days)\", fontsize=18)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum secondary depth we use is Eq. 3 of Morton et al. 2016:\n",
    "\n",
    "$\\delta_{max} = \\delta_{sec} + 3\\sigma_{sec}$\n",
    "\n",
    "where δsec is the fitted depth and σsec is the uncertainty\n",
    "on that depth.\n",
    "When you are preparing for publication, you will want to compute your uncertainty uniquely, but in this tutorial we will just use the scatter in the observed depths (when said depths are computed over a variety of phases).\n",
    "\n",
    "Now, the true secondary eclipse may not be at exactly 0.5 phase, so we need to check the entire non-transiting region and take the deepest event. We do this by repeating the analysis we just did, but assuming that the secondary eclipse may occur at any time, so using a wide variety of 'center' eclipse locations. The deepest 'eclipse' from the results of this grid search will then we taken as the limit of the depth for the secondary. Then, we can compute the uncertainty on the eclipse depth by taking the standard deviation of all these values. \n",
    "\n",
    "We perform this grid search as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the uncertainty on the depth\n",
    "mean_flux_array = []\n",
    "for dayval in np.linspace(-0.4*periods[0], 0.4*periods[0], 100):\n",
    "    offset_phased_time = (epic_target_fitted['Time'].values  - tcent[0] + 2454833) % periods[0] - periods[0] / 2\n",
    "    idx_event = (epic_target_fitted['Offset Time'] > -0.25*durations[0] - dayval) & (epic_target_fitted['Offset Time'] < 0.25*durations[0] - dayval)\n",
    "    mean_flux_array.append(mean(epic_target_fitted.loc[idx_event]['Flattened Relative Flux with planet 1 removed'].values))\n",
    "    # uncomment these to plot the locations you're testing:\n",
    "    #plt.plot(epic_target_fitted.loc[idx_event]['Offset Time'].values,epic_target_fitted.loc[idx_event]['Flattened Relative Flux with planet 1 removed'].values,'r.')\n",
    "    #plt.plot(epic_target_fitted['Offset Time'], epic_target_fitted['Flattened Relative Flux with planet 1 removed'].values, '.', color='k',alpha=0.1, label=\"all data\")\n",
    "    #plt.figure()\n",
    "sigsec = np.std(mean_flux_array) # get overall uncertainty by taking standard deviation of all non-transit flux\n",
    "\n",
    "# choose the deepest depth you found\n",
    "dsec = 1 - min(mean_flux_array)\n",
    "\n",
    "# Compute the secondary limit\n",
    "dmax = dsec + 3 * sigsec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding these parameters to the `fpp.ini` file, you will have a complete file that looks like the example. You can check your work by looking at the star.ini file in the 'Examples' folder, but make sure to give it a try yourself first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SECTHRESH\", dmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value gives the threshold to which a secondary transit can be excluded. You can also add this to `fpp.ini`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the star.ini file\n",
    "You can also use the information on ExoFop to make this file. Recall what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "less TestCase1/star.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file has magnitudes (J, H, K, Kepler; g, r, i, z) and stellar properties derived from a stellar spectrum (Teff, feh, logg), and you should also add the RA/DEC of the source. \n",
    "\n",
    "Create this file from the values on ExoFop, which come from the results of an analysis of the spectra. You can check your work by looking at the star.ini file in the 'Examples' folder, but make sure to give it a try yourself first. \n",
    "\n",
    "Look at the different information available on ExoFop. \n",
    "\n",
    "*If time allows*: How does changing the solution used (there are two spectral analyses for this particular star) change the final result? Try using the 2017 stellar parameters and check by how much that changes the final result of VESPA. Also, see how things change if you take away the spectroscopic contraints and only fit the stellar models using photometry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now, you can run the final VESPA analysis on this system (this might take a while - recall that you can lower the number of iterations using the argument -n X, when X is the number of iterations you want to run.). \n",
    "\n",
    "Make sure the change the directory `Data` to be wherever you put the files you just made. If you're using our premade versions, then use folder `Example`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run in a bash terminal:\n",
    ">`starfit --all Data`\n",
    "\n",
    ">`calcfpp Data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 (if time allows): Now, try to make the files for the other planet in this system. \n",
    "You can use the tutorial above as a starting point, and change the code as needed to derive the probabilities for Planet 2. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, plot the entire light curve and see if you can pick out the transit event. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, phase fold the light curve using the orbital period. Make sure to use the version that has the\n",
    "# other planet removed. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it and decide how much you need to trim.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the light curve; plot it to make sure you did it right, and save it to the transit.txt file. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the fpp.ini file. Can you use the same file that you did for the last planet? What do you need to change about it?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the star.ini file. Can you use the same file that you did for the last planet? What do you need to change about it?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, run the final analysis using starfit and calcfpp.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 2\n",
    "### Extra contraints\n",
    "\n",
    "You may find that you have additional constraints in addition to the measurements discussed above. These additional constraints can also be added into VESPA. \n",
    "\n",
    "## Contrast curves\n",
    "Contrast curves are generated by adaptive optics imaging observations. They inform us the limit on the relative brightness of any nearby sources, and can be used to rule out the presence of nearby stars at certain radii brighter than some amount. The contrast curve quantifies each of those values. \n",
    "\n",
    "For the target we've been studying in Appendix 1, the contrast curve is available [on ExoFop](https://exofop.ipac.caltech.edu/k2/files/248463350/Image/248463350I-dc20171227K_plot.tbl). It has also been copied to your Data directory, but you can look at the online file to see what the fields mean and more details about the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.stars.contrastcurve import ContrastCurveFromFile\n",
    "cc = ContrastCurveFromFile('Data/Keck_k.cc', 'k')\n",
    "cc.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `fpp.ini` file can be fixed to include this curve as a contraint, as well. Currently, your file should contain the properties you derived above (secthresh and maxrad):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "more Example/fpp.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you can add another line at the bottom of the `fpp.ini` that includes the file containing the contrast curve:\n",
    "\n",
    "    ccfiles = Keck_K.cc\n",
    "\n",
    "After which you can run VESPA as normal. NOTE: you must include the contrast curve file in the directory containing your `fpp.ini` and `star.ini` files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add a contrast curve as a constraint if you are loading a previously run false positive calculation and want to add the constrast curve as a further constraint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.stars.contrastcurve import ContrastCurveFromFile\n",
    "from vespa import FPPCalculation\n",
    "\n",
    "f = FPPCalculation.load('Example') #or whichever directory you want to load\n",
    "f.FPP() # Just to see what it is before doing anything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = ContrastCurveFromFile('Data/Keck_k.cc', 'K')\n",
    "bebs = f['beb']\n",
    "bebs.apply_cc(cc)\n",
    "f.FPP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing those two probabilities, you can see how the FPP decreases with the addition of the contrast curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 3\n",
    "### Reading fits files\n",
    "(provided for reference - we will not go through this in the workshop, and projects will use reduced extracted light curves)\n",
    "\n",
    "Above, we used a processed .csv file that contained the light curve from the source. You can also download the light curve from MAST, and if you do that, you get a fits file. Let's take a look at what the contents of that file look like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "epic_fits = fits.open('Data/lc248463350.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what's available in the file:\n",
    "epic_fits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first extension contains the header:\n",
    "epic_fits[0].header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second extension contains the light curve, with a tuple of each data point. \n",
    "epic_fits[1].header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can reformat this into an easily plottable format (check the header of this extension (above) if you need\n",
    "# to know which column is which, and you can try plotting the other columns):\n",
    "time = list(zip(*epic_fits[1].data))[0]\n",
    "flux = list(zip(*epic_fits[1].data))[3] # aperture photometry flux (column #4 above), not corrected\n",
    "plt.plot(time, flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the third extentsion contains the aperture image, which you can look visualize and inspect:\n",
    "imshow(epic_fits[2].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
